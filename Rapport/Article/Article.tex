\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}

\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{verbatim}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{multirow}
\usepackage{url}
%\usepackage[sorting=none]{biblatex}
\usepackage[utf8]{inputenc}
\usepackage{ifthen}
\usepackage{filecontents}
\usetikzlibrary{shapes,arrows,shadings,patterns}
\usepackage{pgfplots}
\pgfplotsset{compat=newest}
\pgfplotsset{plot coordinates/math parser=false}

\makeatletter
\def\BState{\State\hskip-\ALG@thistlm}
\makeatother

\usetikzlibrary{positioning}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
    
\makeatletter
\def\endthebibliography{%
  \def\@noitemerr{\@latex@warning{Empty `thebibliography' environment}}%
  \endlist
}
\makeatother    
    
\makeatletter
\def\thickhline{%
  \noalign{\ifnum0=`}\fi\hrule \@height \thickarrayrulewidth \futurelet
   \reserved@a\@xthickhline}
\def\@xthickhline{\ifx\reserved@a\thickhline
               \vskip\doublerulesep
               \vskip-\thickarrayrulewidth
             \fi
      \ifnum0=`{\fi}}
\makeatother

\newlength{\thickarrayrulewidth}
\setlength{\thickarrayrulewidth}{3\arrayrulewidth}    
    
\begin{document}

\title{Machine Learning Autoencoder Applied to Communication Channels}


\author{
\IEEEauthorblockN{Eduardo Dadalto Camara Gomes\IEEEauthorrefmark{1},
Meryem Benammar\IEEEauthorrefmark{2}}

\IEEEauthorblockA{\IEEEauthorrefmark{1}Institut Supérieur de l'Aéronautique et de l'Espace (ISAE-SUPAERO), Université de Toulouse, 31055 Toulouse, FRANCE\\
Email: eduardo.dadalto-camara-gomes@student.isae-supaero.fr
}
\IEEEauthorblockA{\IEEEauthorrefmark{2}Institut Supérieur de l'Aéronautique et de l'Espace (ISAE-SUPAERO), Université de Toulouse, 31055 Toulouse, FRANCE\\
Email: meryem.benammar@isae-supaero.fr
}
}

\IEEEspecialpapernotice{(Final report)}

\maketitle

\begin{abstract}

Communication channel error correction is key to enable digital critical communication systems to work efficiently. The maximum a posteriori (MAP) decoder is proven to be the mathematically optimal solution for the problem. However, its implementation cripples the system applicability. This is because the MAP rule introduces a large delay for long code words. In this context, a deep neural network (DNN) is proposed to optimize the channel with an end-to-end autoencoder. The trained DNN autoencoder has a \textit{one-shot} capability which outperforms a MAP decoder for a given encoder in terms of delay. ONE PHRASE FOR RESULTS

\end{abstract}

\begin{IEEEkeywords}
communication system, machine learning,  autoencoder, channel decoding, maximum a posteriori (MAP) decoder 
\end{IEEEkeywords}

\section{Introduction}
\subsection{Motivation}
A point to point communication channel is a system in which  two terminals exchange information through a noisy channel as Fig. \ref{fig:cs} illustrates. As a result of channel imperfections, Shannon theorized in \cite{Shannon:2001:MTC:584091.584093} the ultimate reliable data rate that can be transmitted through a communication system with an arbitrarily small probability error. 

\begin{figure}[!ht]
  \centering
    \includegraphics[width=0.5\textwidth]{simple_sys}
    \caption{Diagram of a simplified communication system.}\label{fig:cs}
\end{figure}

Since then, the research community in digital communication developed a series of algorithms to minimize error probability over a channel. In practice, the bit error rate (BER), an approximation of the probability of error, is targeted. However, the challenge of finding an \textit{efficient} solution  i.e., with both low latency and low error probability, for low signal to noise ratio (SNR) channels, remains. Low latency and high bandwidth wireless communication are key to critical systems, such as airplanes, satellites, cellular communication and 5G operations. The latter was studied by F. D. Calabrese et al. in \cite{DBLP:journals/corr/CalabreseWGPS16} which  demonstrated that individual based radio resource management (RRM) algorithms were outperformed by a general learning framework, resulting in significant expense reductions, while increasing performance of the network.

Based on this vision, Tim O'Shea and Jakob Hoydis \cite{DBLP:journals/corr/OSheaH17} pertinently noted that traditional algorithms in the field have foundations in probability theory e.g. maximum a posteriori (MAP) rule, maximum likelihood decoder (MLD) and turbo codes. Hence, they are usually built on top of mathematically convenient models. Even though they are theoretically optimal error corrector codes, these models often do not account for all the real system's imperfections, which leads to errors when implemented in practice.

As opposite to structured algorithms, machine learning (ML) algorithms do not require rigidly designed models and can take non-linearities effortlessly into account. These characteristics make these algorithms candidates for being used as channel decoder. Moreover, with ML based channel encoder and channel decoder, the design of communication systems as independently working blocks becomes obsolete, as a deep neural network (DNN) is able to actuate end-to-end in the system. As a result, an optimized autoencoder with a stochastic layer that models channel's imperfections can substitute the block based representation. Therefore, ML based communication systems could be a better representation of realistic systems and could optimize information transmission of different blocklengths and with low decoding latency, resulting ultimately in gain of bandwidth over standard methods. Thereby, ML algorithms are cardinal for state of the art communication applications. 

\subsection{Related work}

Recently, a significant amount of work in radio communication theory has emerged, introducing ML elements to the communication system. O'Shea et al. in \cite{2016arXiv160806409O} developed a channel autoencoder with optimized impairment and regularization layers to emulate channel impairments. They studied this architecture over an additive white Gaussian noise (AWGN) channel, founding ``some promising initial capacity" for this scheme. In their research, results in terms of BER over SNR for a DNN based autoencoder and for a convolutional neural network (CNN) based autoencoder were treated. They used a range of SNR - from $-10 \text{dB}$ to $15 \text{dB}$ - with QPSK and QAM16 modulation as benchmarks. This analysis was conducted for a binary input message.

    
T. Gruber et. al. in \cite{2017arXiv171008379G} proved that a deep learning-based channel decoder could actually learn a decoding algorithm rather than just being a simple classifier. They introduced code words that were not been used in the training set, and the trained NN was able to correctly decode it. They also observed that structured codes are easier to learn than unstructured ones. NN for structured codes are able to generalize to the full codebook even if they have not seen all the training examples. They trained the NN for very short blocklengths ($N \leq 64$) in order to compare with MAP decoding performance.

\subsection{Problem statement}

In this work, we will implement a ML autoencoder for a  BSC that performs similar to the MAP decoder in real applications for a range of SNR from $-10 \text{dB}$ to $10 \text{dB}$. In its most simple form, a channel autoencoder includes an encoder, a noisy channel and a decoder. Using state-of-the art DNN algorithms to find the best solution of the problem, this work aims to contribute to set up a higher standard in terms of performance in bit-error correction and reduced delay for digital communication applications. In a near future, this disruptive methodology for error correction using ML could replace mathematically optimal decoders which are the current guideline.


\subsection{Notation}
Throughout the work, vectors will be written in bold font weight and may  have a subscript which indicates their layer in a NN and may have a superscript which indicates their size length. In order to homogenize notation, for the encoding phase of the communication system, $\textbf{u}^k$ represents an input \textit{source message} and $\textbf{x}^n$ its correspondent code word. For the decoding portion, $\textbf{y}^n$ represents the output of the noisy channel and $\hat{\textbf{u}}^k$ the estimated source message. These definitions are illustrated in Fig. \ref{fig:cs}.

In addition, every indispensable concept is introduced either in italics or is defined by a commonly used abbreviation present in others scientific works.


\section{Theoretical Background}

\subsection{Channel coding}
Consider the communication system illustrated in Fig. \ref{fig:cs}. The left portion of the chain, the transmitter, wants to communicate a message $\textbf{u}$ through a noisy channel. The right portion of the chain, the receiver, may interpret this possible corrupted stream of bits into the original message. Usually this prediction $\hat{\textbf{u}}$ carry errors which we aim to minimize its probability of occurring. In this scenario, the theory of channel coding discuss possible solutions. 

In particular, linear block codes represented by a pair $(n, k)$ called \textit{code name} will be implemented. Where $k$ is the length of a source message and $n$ is the length of a code word. Briefly, a linear encoder will index a message $\textbf{u}^k \in \mathcal{U} \subseteq \{0,1\}^k$ to a code word $\textbf{x}^n \in \mathcal{X} \subseteq \{0,1\}^n$. This mapping is done by the \textit{generator matrix} $ \textbf{G} $.
\begin{equation}\label{eq:encoding}
\mathbb{E}:\textbf{u} \mapsto \textbf{uG}
\end{equation}

The objective of the receiver is to estimate the original message. The empirical error probability, or a BER, in this process is defined as

\begin{equation}
P_{eb} = \frac{1}{M} \underset{u}{\sum}Pr(\hat{u}\neq u)
\end{equation}
where $M$ is the total amount of bits transmitted, $u$ represents one bit transmitted and $\hat{u}$ its estimation.

The Hamming distance $d_H:\mathcal{X}\mapsto \mathbb{Z}$ is a metric in the word subset which in the context of linear coding is equal to equation \ref{eq:dh}. Noteworthy for the MAP rule, a non zero Hamming distance indicates that there is a block error, and ultimately at least a binary error in the received code word.
\begin{equation}\label{eq:dh}
d_H(\textbf{x},\textbf{y}) = \sum_{i=1}^{n} y_i \text{ XOR } x_i 
\end{equation}  

\subsection{Binary Symmetric Channel}

Briefly, a BSC is a noisy channel with a binary input $X\in{0,1}$ and a binary output $Y\in{0,1}$. They are linked through a stochastic model characterized by a crossover probability of value $p$ as illustrated in Fig. \ref{fig:BSC}. Typically $p$ is smaller than $0.10$.


\begin{figure}[!ht]
  \centering
    \includegraphics[width=0.5\textwidth]{images/BSC}
    \caption{Illustration of a binary symmetric channel with crossover probability $p$.}\label{fig:BSC}
\end{figure}

A BSC is translated mathematically as follows.

\begin{align*}
Pr[ Y = 0 | X = 0 ] &= 1 - p \\
Pr[ Y = 0 | X = 1 ] &= p \\
Pr[ Y = 1 | X = 0 ] &= p \\
Pr[ Y = 1 | X = 1 ] &= 1 - p
\end{align*}

\subsection{Maximum a posteriori decoder}


The concept of a MAP decoder algorithm for sequences, is choosing a message which maximizes the a posterior probability of the corrupted code word $y$ received by the decoder \cite{Worm00turbo-decodingwithout}. This algorithm is known for its optimal error correction capability for white noise interference. This algorithm is also referred as the Viterbi decoder \cite{Viterbi}.

Mathematically, we want to maximize the average probability of making a correct decision when analysing the channel output $\textbf{y}$, which is written as
\begin{equation}\label{eq:MAP1}
Pc := Pr[\textbf{X}=f({Y})]
\end{equation}
where $x \sim X$ and $y \sim Y$ are multivariate random variables. We derive below the decoder that maximizes $P_c$.

\begin{align} 
\Pr \left[ X=f\left( Y\right) \right]& =\sum_{f\left( y\right) y\in \mathcal{X}\times \mathcal{Y}} P_{XY}(f(y)y) \label{eq:MAP2} \\
&=\sum _{y\in \mathcal{Y}}P_{Y}\left( y\right)P_{X|Y}\left( f(y)|y\right) \label{eq:MAP3}
\end{align}

The optimal decoder is finally given by
\begin{align}
f(y) &= \underset{x\in \mathcal{X}}{\text{arg max}} P_{X|Y}(x|y)\label{eq:MAPF1}\\
&= \underset{x\in \mathcal{X}}{\text{arg max}}
P_X(x)P_{Y|X}(y|x).\label{eq:MAPF2}
\end{align}

However, the MAP decoder is practically nonviable to implement \cite{journals/ett/RobertsonHV97}. Thus, its variants cited in this paragraph are better suited for practical cases. The Log-MAP is an optimal decoder, having equivalent performance to the MAP decoder. While the Max-Log-MAP is a suboptimal decoder and will not be treated in this paper.  these variants are widely used as decoders for turbo codes \cite{HagenauerJ}. Refer to \cite{journals/ett/RobertsonHV97} for specific details in the derivation of the Log-MAP algorithm. Furthermore, the Bahl-Cocke-Jelinek-Raviv (BCJR) algorithm is a bit-wise MAP rule with lower complexity widely used in practice, even for long data packets.

Turbo decoding using Log-MAP decoder for an additive white Gaussian noise (AWGN) channel and for a BSC was studied in \cite{JordanMA}. They analyzed the BER for small SNR and concluded that the results are sensible to SNR estimation of the real channel, since the estimation of this parameter is required for the metric calculation of the algorithm \cite{journals/ett/RobertsonHV97}. It means that if the real SNR is smaller, the decoder will not perform well. Hence, for channel characteristics that change over time, the application of a MAP algorithm is debatable. This conclusion serve as motivation for ML autoencoders, which could adapt to different SNR conditions, outperforming the MAP decoder for realistic applications.

\subsection{Neural network basics}

\tikzset{%
  every neuron/.style={
    circle,
    draw,
    minimum size=0.666cm
  },
  neuron missing/.style={
    draw=none, 
    scale=2,
    text height=0.333cm,
    execute at begin node=\color{black}$\vdots$
  },
}

\begin{figure}[!hbp]
\centering

\begin{tikzpicture}[x=1.1cm, y=1.1cm, >=stealth]

\foreach \m/\l [count=\y] in {1,2,3,missing,4}
  \node [every neuron/.try, neuron \m/.try] (input-\m) at (0,2.5-\y) {};

\foreach \m [count=\y] in {1,missing,2}
  \node [every neuron/.try, neuron \m/.try ] (hidden-\m) at (2,2-\y*1.25) {};

\foreach \m [count=\y] in {1,missing,2}
  \node [every neuron/.try, neuron \m/.try ] (output-\m) at (4,1.5-\y) {};

\foreach \l [count=\i] in {1,2,3,n}
  \draw [<-] (input-\i) -- ++(-1,0)
    node [above, midway] {$y_\l$};

\foreach \l [count=\i] in {1,j}
  \node [above] at (hidden-\i.north) {$r_{l,\l}$};

\foreach \l [count=\i] in {1,k}
  \draw [->] (output-\i) -- ++(1,0)
    node [above, midway] {$\hat{u}_\l$};

\foreach \i in {1,...,4}
  \foreach \j in {1,...,2}
    \draw [->] (input-\i) -- (hidden-\j);

\foreach \i in {1,...,2}
  \foreach \j in {1,...,2}
    \draw [->] (hidden-\i) -- (output-\j);

\foreach \l [count=\x from 0] in {Input, Hidden, Ouput}
  \node [align=center, above] at (\x*2,2) {\l \\ layer};

\end{tikzpicture}

\caption{MLNN representative diagram, where $\textbf{y}^n$ is the input vector, $\textbf{r}_{l}^{j}$ is a hidden layer vector
and $\hat{\textbf{u}}^{k}$ is the output vector.} \label{fig:NN}
\end{figure}

Neural networks (NN) are a set of connected \textit{neurons} able to approximate any kind of function through an architecture composed of several single processing units (neurons) connected together forming a network. They are defined by \cite{Ibnkahla} as parallel distributed, learning- and self-organizing information processing systems. 

In the context of learning algorithms implemented by a NN, the key issue is to find a suitable architecture that delivers the best results. To define and build this infrastructure, the activation function, the loss function and the structure must be decided. Nielson \cite{nielsenneural} in his book explores a vast amount of NN architectures and how they work. 

Multi-layer feed forward neural network (MLNN) as shown in Fig. \ref{fig:NN} will be applied in this work. They are known for the universal approximation property \cite{Ibnkahla} and for the versatility of increasing the number of layers, creating a DNN able to undertake complex classification problems with low classification error and low dimensionality. In more general terms, a MLNN with $L$ layers, also called \textit{depth}, and parameter $\theta$ is a mapping of an input $\textbf{y}^n \in \mathbb{R}^{n}$ to an output $\textbf{û}^k \in \mathbb{R}^{k}$ through $L$ iterative steps. For a fully-connected network, each layer vector is calculated iteratively in the forward propagation. Each neuron is composed of a linear combiner and an activation function defined as 

\begin{equation}\label{eq:eqFP}
	f_{l}\left( \textbf{r}_{l-1};\theta _{l}\right) = \sigma \left( \textbf{W}_{l}\textbf{r}_{l-1}+\textbf{b}_{l}\right)
\end{equation}
where $\textbf{W}_{l}\in \mathbb{R} ^{N_{l}\times N_{l-1}}$ is the weight matrix between layers $l-1$ and $l$, $\textbf{r}_{l-1} \in \mathbb{R} ^{N_{l-1}}$ is a vector containing the hidden layer $l-1$ values, $\textbf{b}_{l}\in \mathbb{R} ^{N_l}$ is the bias vector and $\sigma$ is the activation function \cite{DBLP:journals/corr/OSheaH17}.

% TODO: WRITE WHICH ACTIVATION FUNCTION I WILL USE
%
Two common activation functions are the rectified linear unit (ReLU) and the sigmoid function. Both will be applied to the DNNs treated in this paper. In addition, the Softmax activation function is commonly used in multi classification problems and will be used to the One-hot decoding DNN architectures. Their mathematical expressions are shown below.

\begin{align}
	\text{ReLU: } \sigma_1 (x)& = max\{0, x\} \label{eq:relu}\\ 
	\text{Sigmoid: } \sigma_2 (x)& = \frac{1}{1+e^{- \lambda x}} \label{eq:sig}\\
	\text{Softmax: } \sigma_3 (\textbf{z})& = \frac{e^{z_i}}{\sum_{j=1}^{K}e^{z_j}}\label{eq:soft}
\end{align}
where $x \in \mathbb{R}$, $\lambda>0$, $i = 1,...,K$ and $\textbf{z} = (z_1,...,z_K) \in \mathbb{R}^K$.

For a classification problem in communication, the \textit{categorical cross-entropy} loss function $l(\textbf{p},\textbf{q}):\mathbb{R} ^{N_L}\times \mathbb{R} ^{N_L}\mapsto \mathbb{R}$ defined in \eqref{eq:c-e} is most common. It is derived from the log-likelihood of a training set where $q_j$ is the estimated probability of the outcome $j$ and $p_{j}$ is the true probability. Basically, the function measures the dissimilarity between $p_{j}$ (what you expected) and $q_{j}$ (what you obtained) \cite{murphy2013machine} where $j=1,...,N_L$.
\begin{equation}\label{eq:c-e}
	l(\textbf{p},\textbf{q})=-\sum _{j}p_{j}\log \left( q_{j}\right)
\end{equation}

With all these elements set, training the neural network to calculate an accurate weight matrix $\textbf{W}$ for forward propagation requires a labeled \textit{training data set}. This set is composed of pairs $ (\textbf{y}_{i}^{n}, {\mathbf{u}}_i^k) $, where $i=1,...,S$ and $S$ is the number of training sets. It matches the input and its respective desired output. The objective is to minimize the overall loss function defined in \eqref{eq:o-c-e} in terms of the parameter $\theta$ \cite{DBLP:journals/corr/OSheaH17}. 
\begin{equation}\label{eq:o-c-e}
L\left( \theta \right) =\dfrac {1}{S}\sum ^{S}_{i=1}l\left({\mathbf{u}}_i^k  , \textbf{y}_{i}^{n}\right)	
\end{equation}  

This problem is an optimization problem and can be solved through different algorithms. For NN, an efficient way of computing this minimization is implementing the back-propagation (BP) algorithm. 	BP is classified as a supervised learning algorithm that is able to optimize a function based on some parameter and is highly parallelizable in computational terms \cite{Ibnkahla}\cite{nielsenneural}.

One of the greatest advantage of NN is its ability to generalize the training set, delivering the right results even for input data not contained in the training set. With the weight matrix trained, the NN finds the correct output values for unseen inputs. In order to validate this principle, a \textit{validation data set} is used and the loss is measured. Based on this value, we can evaluate the NN overall accuracy.

For implementing all the algorithms, Keras, a high-level python API for ML, will be used together with TensorFlow 1.13.1, a ML python friendly open-source library \cite{DBLP:journals/corr/AbadiABBCCCDDDG16} \cite{chollet2015keras}. Both are available on a free license terms.




\subsection{Autoencoders}

Autoencoders based on DNN can learn an appropriate correspondence between input source messages and estimated source message output through different noisy channels. The statistics of the channel's noise is shown in \cite{2017arXiv171008379G} to be irrelevant, since the NN extract it during training phase. The autoencoder applicability is compelling because the weights computed during training can be separated in two groups, one correspondent to the nodes before the channel and one posterior to the channel. As a consequence, we are able to design a non linear encoder and its correspondent non linear decoder with ease. This new representation has ultimately far more possibilities than conventional encoding patterns.

Mathematically, the non linear autoencoder`s encoding function $\phi: \{0,1\}^k \mapsto \{0,1\}^n $ maps the message input to its code word. 

\begin{equation}
x^n = \phi (u^k)
\end{equation} 

Whilst autoencoders are more complex dimensional wise compared to NN decoders, the readily access to graphical processing units (GPUs) contributed to the feasibility of this technique, tackling the \textit{curse of dimensionality} challenge in ML. This type of hardware made possible the approach of the layer-by-layer supervised training by stochastic gradient descent optimization applied in this work. \cite{doi:10.1162/neco.2006.18.7.1527}

A common architecture of DNN autoencoders is shown in Fig. \ref{fig:DDNNAutoencoder} where the encoder and the decoder are composed of dense layers of different widths. In addition, a noise layer and a rounding layer acts on the forward propagation but not on the BP, since their activation functions are not differentiable. More details on the autoencoder's architecture is discussed is section III.  


\begin{figure}[!ht]
  \centering
    \includegraphics[width=0.4\textwidth]{images/DNN_autoencoder}
    \caption{Representation of a DNN autoencoder composed of dense layers.}\label{fig:DDNNAutoencoder}
\end{figure}



\section{Implementation and Methodology}
This section explains the main implementation procedures in order to replicate the results and understand in depth the methodology behind the work. For organizational purposes, the section is divided into four subsections. First, the predictions and error corrections statistical relevance are justified through the Monte Carlo simulations conducted. Second, the MAP algorithm for a linear code is shown. Then the experiments relating to the ML based decoders are assessed. Finally the implementation of the ML based autoencoder is explained. 

\subsection{Monte Carlo Simulation}

For calculating the BER versus the crossover probability $p$ with confidence over the results, a Monte Carlo simulation was elaborated. This stochastic computational algorithm uses pseudo-random sampling to solve different problems. Hence, we can work with a wide variety of transmission scenarios.

Applied to the context of this research, the messages are repeatedly randomly generated for a specific BSC of crossover probability $p$. They are coded by a linear block code or by a DNN encoder and then transmitted through the channel. Their information arrives corrupted by noise in the decoder and is subsequently partially corrected by a decoder. Finally the average BER is calculated and plotted. The interval of the horizontal axis is in between $0$ and $0.10$.

Every channel is tested with $100000$ messages. As a consequence of the \textit{law of large numbers}, the empirical sampling of the BER represents sufficiently well the real distribution. 

\subsection{MAP Rule}

The MAP algorithm as cited previously, has a high time complexity $(\mathcal{O}(2^k))$, since it computes the Hamming distance between the received code word and every code word from the code book. For more details, the pseudo code of the algorithm is available below.

\begin{algorithm}
\caption{MAP rule for BSC and linear block code.}\label{alg:MAP}
\hspace*{\algorithmicindent} \textbf{Input:} received block $\textbf{y}^n \in \{0,1\}^n$, code word set $\mathcal{X}$ and generator matrix $G_{k \times n}$. \\
\hspace*{\algorithmicindent} \textbf{Output:} message estimation $\hat{\textbf{u}}^k \in \{0,1\}^k$.
\begin{algorithmic}[H]
\Procedure{MAP Decoder}{$y,\mathcal{X}, G$}
\State $\textit{p} \gets \text{channel crossover probability}$
\For{i in \textit{range}($2^k$)}
\State $\text{distances}[i] \gets d_H(\textbf{y},word[i] \in \mathcal{X})$
\EndFor
\State $\hat{\textbf{x}} \gets argmin(\text{distances})$
\State $\hat{\textbf{u}} \gets \hat{\textbf{x}} G^{-1}$
\Return $\hat{\textbf{u}}$
\EndProcedure
\end{algorithmic}
\end{algorithm}

The linear encoding matrix $\textbf{G}_{8\times16}$ was used for the encoding of the messages. Its dimension is equal to the linear block code name represented by the double $(n,k)=(16,8)$. Thus, the \textit{rate} of the code is equal to $0.5$. The linear block name and = rate is constant throughout the work.


\begin{flalign*}
    &\textbf{G}=&
\end{flalign*}

\[
\left [
  \begin{array}{cccccccccccccccc}
  
   1 & 1 & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
	1 & 1 & 0 & 0 & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
	1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
	1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\
	1 & 1 & 1 & 1 & 0 & 0 & 0 & 0 & 1 & 1 & 1 & 1 & 0 & 0 & 0 & 0 \\
	1 & 1 & 0 & 0 & 1 & 1 & 0 & 0 & 1 & 1 & 0 & 0 & 1 & 1 & 0 & 0 \\
	1 & 0 & 1 & 0 & 1 & 0 & 1 & 0 & 1 & 0 & 1 & 0 & 1 & 0 & 1 & 0 \\
	1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1
  \end{array}
\right ]
\]

To establish a reference model to use as comparison base to the DNN models and MAP decoder, the BER results for a scenario without codification was calculated. The ensemble of the no decoding curve and the MAP algorithm curve of Fig. \ref{fig:MAP} are replicated for comparison purposes with the methods of interest.

\begin{figure}[!ht]
  \centering
    \includegraphics[width=0.5\textwidth]{images/MAP-alone-curve}
    \caption{No decoding and MAP algorithm performance in terms of BER versus channel crossover probability.}\label{fig:MAP}
\end{figure}

\subsection{Decoder}

This section is divided in two for better identify the differences between the implementation of the array DNN decoder and the one-hot DNN decoder.

\subsubsection{Array Decoder}

The architecture of the DNN necessary to achieve a great performance in terms of BER is available in table \ref{tab:arraydecoder}. What should be remarked is the first layer of the DNN. A \textit{Lambda} layer is a layer in which the activation operations are user defined and the parameters are non-trainable. Thus, we were able to define a layer that act as a BSC to force our DNN to learn its effects in the training data. The fact that the parameters are non-trainable means that the noise layer is applied in forward propagation only.

The choice of the crossover probability of the training channel, $p_{t}$, was based on \cite{DBLP:conf/acssc/BenammarP18} which shows $p_{t}=0.07$ to be the best in terms of decoding precision after the DNN is trained. In addition, a deeper NN showed best results in \cite{ DBLP:journals/corr/OSheaH17} and were adopted to solve the problem. The training parameters are displayed in table \ref{tab:arraydecoderTrain}.

The choices of training parameters is a mix between what other works in literature uses and a hyper parametric analytic approach, where several trials lead to the decision. First, the binary cross-entropy as loss function is widely used in literature and  showed to converge to the desired result consistently during tests. For the optimizer, the \textit{Adam} built-in Keras optimizer was chosen. Adam optimizer algorithm is similar to a classic stochastic gradient descent algorithm, but computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients \cite{chollet2015keras}. Next, the BATCH SIZE. Finally, the necessary number of epochs to training is sensitive to the problem, since it impacts directly the training computational cost. As a result, we tried several training epoch sizes and found $2^{16}$ to have precise results and be computationally viable.

\begin{table}
\caption{DNN array decoder architecture.} 
\begin{center}

  \begin{tabular}{ l  |  l }
    \thickhline
    Channel &  Lambda: $\textbf{x}\oplus \text{noise}$\\ \hline
    \multirow{3}{*}{Decoder} & Dense: 128, activation: ReLU, input size: 16 \\
 & Dense: 64, activation: ReLU  \\
 & Dense: 32, activation: ReLU  \\
 & Dense: 8, activation: Sigmoid  \\ \hline
    \multicolumn{2}{l}{\textbf{Total parameters: $12776$}}\\
    \thickhline
  \end{tabular}
    \label{tab:arraydecoder}
\end{center}
\end{table}

\begin{table}
\caption{DNN array decoder training parameters.} 
\begin{center}
\begin{tabular}{ l  |  l | l | l  }
    \thickhline
 Loss func. & Optimizer & N. Epochs & Batch Size \\ \hline
 Binary cross-entropy & Adam & $2^{16}$ & $256$ \\
    \thickhline
  \end{tabular}
    \label{tab:arraydecoderTrain}
\end{center}
\end{table}

\subsubsection{One-hot Decoder}

The architecture of the one-hot decoder is much simpler when compared to the array decoder. It contains only one layer of $256$ units which maps the input of size 16 to the $2^8=256$ possible messages. Hence, this characterizes a multi-class classification problem, meaning that the chosen activation function is the Softmax (Eq. \ref{eq:soft}). This training technique is known in the literature as \textit{one-hot}. 

The denomination one-hot comes from the output vector which is usually a long vector of zeros with one element equals to one. This \textit{hot} vector element indicates which class the NN has classified the input. In this context, which message encoded in 256 different classes the code word refers to. Basically, a message is mapped into a class and a class only.

A straight forward consequence of this shallower NN is the number of training parameters almost three times less than the array decoder. Because of that, we could expect a lower training time. Also, we observed that the number of training epochs is substantially smaller for the one-hot decoder. It was necessary only $2^{14}$ epochs to achieve the results for the same batch size of $256$ and Adam optimizer.

Despite the NN complexity benefits of the one-hot decoder, it is required to transform the predicted vector into a message. Consequently, an additional operation is needed. To transform from a one-hot vector to a message, a ordered collection of the messages was stored in memory and the correspondent message is determined by a $argmax$ operation in the one-hot vector, which identifies which message is the most likely to have been transmitted through the channel.

The one-hot decoder was trained both with and without a noise layer. The model without a integrated channel converged faster to the solution. Tables \ref{tab:onehotdecoder} and \ref{tab:onehotdecoderTrain} resume all the training variables.

\begin{table}
\caption{DNN one-hot decoder architecture.} 
\begin{center}

  \begin{tabular}{ l  |  l }
    \thickhline
    \multirow{1}{*}{Decoder} & Dense: 256, activation: Softmax, input size: 16 \\ \hline
    \multicolumn{2}{l}{\textbf{Total parameters: }$4352$}\\
    \thickhline
  \end{tabular}
    \label{tab:onehotdecoder}
\end{center}
\end{table}

\begin{table}
\caption{DNN one-hot decoder training parameters.} 
\begin{center}
\begin{tabular}{ l  |  l | l | l  }
    \thickhline
 Loss func. & Optimizer & N. Epochs & Batch Size \\ \hline
 Binary cross-entropy & Adam & $2^{14}$ & $256$ \\
    \thickhline
  \end{tabular}
    \label{tab:onehotdecoderTrain}
\end{center}
\end{table}

\subsection{Autoencoder}
Architecture, Training and simulation parameters, procedure to find a M epoch suitable, choices of p train, Talk about NN separation, 

\begin{table}
\caption{DNN array autoencoder architecture.} 
\begin{center}

  \begin{tabular}{ l  |  l }
    \thickhline
    \multirow{3}{*}{Encoder} & Dense: X, activation: ReLU, input size: 8 \\
 & Dense: X, activation: ReLU  \\
 & Dense: 16, activation: Sigmoid  \\ \hline
 
    \multirow{2}{*}{Channel} & Lambda: $Round(\textbf{x})$, input size: $n$ \\
 & Lambda: $\textbf{x}\oplus \text{noise}$\\ \hline
 
 \multirow{2}{*}{Decoder} & Dense: X, input size: 16 \\
 & Dense: X, activation: ReLU  \\
 & Dense: 8, activation: Sigmoid  \\ \hline
 
    \multicolumn{2}{l}{\textbf{Total parameters: }XX}\\
    \thickhline
  \end{tabular}
    \label{tab:arrayautoencoder}
\end{center}
\end{table}

\begin{table}
\caption{DNN array autodecoder training parameters.} 
\begin{center}
\begin{tabular}{ l  |  l | l | l  }
    \thickhline
 Loss func. & Optimizer & N. Epochs & Batch Size \\ \hline
 Binary cross-entropy & Adam & $2^{14}$ & $256$ \\
    \thickhline
  \end{tabular}
    \label{tab:arrayautoencoderTrain}
\end{center}
\end{table}


\section{Results and Analysis}

\subsection{Decoders}
BER Curves from both decoders, analyse why the number of epochs is so different. 

\begin{figure}[!ht]
  \centering
    \includegraphics[width=0.5\textwidth]{images/MLNN_Mep_65536_ptrain_007}
    \caption{Array decoding BER performance. NN trained with a channel crossover probability error of $p_t=0.07$.}\label{fig:ArrayD}
\end{figure}

\begin{figure}[!ht]
  \centering
    \includegraphics[width=0.5\textwidth]{images/MLNN1H_Mep_16384_ptrain_0}
    \caption{One hot decoding BER performance. NN decoder trained with a channel crossover probability error of $p_t=0$.}\label{fig:1HD}
\end{figure}


\subsection{Autoencoder}
BER Curves of simulation varying p, best choice of p, analyse autoencoder with best choice of p


\subsection{Decoding time analysis}
compare time in decoding between map, autoencoder and a decoder.

\begin{table}
\caption{Decoding time comparison between the MAP algorithm and the DNN decoders and autoencoders. The data is normalized to the mean MAP algorithm decoding time. } 
\begin{center}

  \begin{tabular}{ l | l | l | l | l }
    \thickhline
    MAP & A. Dec. & One-hot Dec. & A. Auto. & One-hot Auto. \\ \hline
    $1.00 \pm 0.02$ & $0.74 \pm 0.03$ & $0.76 \pm 0.02$ & $ \pm $ & $ \pm $ \\ 
    \thickhline
  \end{tabular}
    \label{tab:timeanalysis}
\end{center}
\end{table}

\section{Conclusions}


\section*{Acknowledgment}
The authors would like to thank...

\cite{Shannon:2001:MTC:584091.584093, DBLP:journals/corr/CalabreseWGPS16, DBLP:journals/corr/OSheaH17, 2016arXiv160806409O, 2017arXiv171008379G, Worm00turbo-decodingwithout, Viterbi, journals/ett/RobertsonHV97, HagenauerJ, JordanMA,Ibnkahla, nielsenneural, murphy2013machine, DBLP:journals/corr/AbadiABBCCCDDDG16, chollet2015keras, doi:10.1162/neco.2006.18.7.1527, DBLP:conf/acssc/BenammarP18}

\bibliographystyle{ieeetr}
\bibliography{bib/ShannonCE1948,bib/CalabreseWGPS16,bib/OSheaH17,bib/Oshea2,bib/Gruber,bib/Worm,bib/Viterbi,bib/RobertsonHV97,bib/HagenauerJ,bib/JordanMA,bib/Ibnkahla,bib/Nielsen,bib/Murphy,bib/AbadiABBCCCDDDG16,bib/Keras,bib/mit_neco18_1527,bib/BenammarP18}

%\bibitem{b1} C. E. Shannon, ``A mathematical theory of communication," Bell System Technical Journal, vol. 27, no. 3, pp. 379–423, Jul. 1948.


%\bibitem{b3} F. D. Calabrese, L. Wang, E. Ghadimi, G. Peters, and P. Soldati, ``Learning radio resource management in 5G networks: Framework, opportunities and challenges," arXiv preprint arXiv:1611.10253, 2016.

%\bibitem{b2} O'Shea, Tim \& Hoydis, Jakob, ``An Introduction to Machine Learning Communications Systems", 2017.

%\bibitem{osheaautoencoder} T. O'Shea, K. arra, T. C. Clancy, ``Learning to Communicate: Channel Auto-encoders, Domain Specific Regularizers, and Attention", 2016.

%\bibitem{2018} T. Gruber, S. Cammerer, J. Hoydis, and S. ten Brink, “On deep learning based channel decoding,” accepted for CISS 2017, arXiv preprint arXiv:1701.07738, 2017.

%\bibitem{b4} W. Alexander, H. Peter \& W. Norbert, ``Turbo-Decoding Without SNR Estimation",IEEE Communications Letter, vol. 4, no. 6, pp. 193-195, 2000.

%\bibitem{viterbi} A. J. Viterbi, ``Error Bounds for Convolutional Codes and an Asymptotically Optimum Decoding Algorithm", IEEE Transactions on Information Theory, 13 (2): 260–269, 1967.

%\bibitem{b6} Robertson, P. , Hoeher, P. and Villebrun, E. (1997), Optimal and sub‐optimal maximum a posteriori algorithms suitable for turbo decoding. Eur. Trans. Telecomm., 8: 119-125. doi:10.1002/ett.4460080202.

%\bibitem{b7} J. Hagenauer,P. Robertson, L. Papke: iterative (“Turbo”) decoding of systematic convolutional codes with the MAP and SOVA algorithms. In: ITG-Fachbericht 130. October 1994, p. 21-29.

%\bibitem{b5} M. Jordan and R. Nichols, “The effects of channel characteristics on turbo code performance,” in Proc. Milcom’96, McLean, VA, Oct. 1996, pp. 17–21.

%\bibitem{b9} I. Mohamed, ``Applications of neural networks to digital communications - a survey", 1997.

%\bibitem{b8} M. Nielsen, \textit{Neural Networks and Deep Learning}.

%%\bibitem{b10} M. Kevin, ``Machine Learning: A Probabilistic Perspective", MIT, ISBN 978-0262018029, 2012.

%\bibitem{b11} M. Abadi, A. Agarwal, P. Barham, et al.,``TensorFlow: Large-scale machine learning on heterogeneous systems", 2015. Software available from tensorflow.org.

%\bibitem{b12} F. Chollet, “keras,” https://github.com/fchollet/keras, 2015.

%\bibitem{GPU} G. E. Hinton, S. Osindero, and Y.-W. Teh, “A fast learning algorithm for deep belief nets,” Neural Computation, vol. 18, no. 7, pp. 1527–1554, July 2006.

%\bibitem{b13} M. Benammar and P. Piantanida, “Optimal training channel statistics for neural-based decoders,” in Asilomar, Oct 2018, pp. 2157–2161.




\end{document}
